{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761b067f",
   "metadata": {},
   "source": [
    "# CV_Assignment-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c380a",
   "metadata": {},
   "source": [
    "1. Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24539ee3",
   "metadata": {},
   "source": [
    "The InceptionNet architecture is a deep neural network with repeating components called inception modules. The inception module consists of multiple convolutional layers with different filter sizes and pooling operations, which are concatenated together to form a single output. This allows the network to extract various feature maps concurrently and perform parallel processing. The Inception V3 model is based on Convolutional Neural Networks and is used for image classification. It achieved a milestone in CNN classifiers when previous models were just going"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863f10d",
   "metadata": {},
   "source": [
    "2. Describe the Inception block ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5337a",
   "metadata": {},
   "source": [
    "The inception block is a component of the InceptionNet architecture that consists of multiple convolutional layers with different filter sizes and pooling operations, which are concatenated together to form a single output. The goal of the inception block is to approximate an optimal local sparse structure in a CNN and allow for more efficient computation and deeper networks through dimensionality reduction. The GoogLeNet architecture has 22 layers in total, including dimension-reduced inception modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284fe9b",
   "metadata": {},
   "source": [
    "3. What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa8ea4",
   "metadata": {},
   "source": [
    "The dimensionality reduction layer in the Inception block is a 1-layer convolutional layer. This layer reduces the number of parameters and computations, allowing for more efficient computation and deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62526bf2",
   "metadata": {},
   "source": [
    "4. THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc781ed5",
   "metadata": {},
   "source": [
    "Reducing dimensionality can have a significant impact on network performance by removing redundant features, noisy and irrelevant data, and improving learning feature. It can help reduce the costs of training and running artificial neural networks, prevent overfitting, improve accuracy, and speed up computation time. However, it may also result in loss of information or important features if not done properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7139a79",
   "metadata": {},
   "source": [
    "5. Mention three components. Style GoogLeNet ?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557fdfc",
   "metadata": {},
   "source": [
    "The GoogLeNet architecture consists of 22 layers (27 layers including pooling layers), and part of these layers are a total of 9 inception modules. It utilises Inception modules, which allow the network to choose the best combination of filters for each layer. The inception block consists of four parallel branches, with the first three branches using convolutional layers with window sizes of 1x1, 3x3, and 5x5 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a51d8",
   "metadata": {},
   "source": [
    "6. Using our own terms and diagrams, explain RESNET ARCHITECTURE ?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf60e21",
   "metadata": {},
   "source": [
    "The ResNet architecture is a type of artificial neural network that allows the model to skip layers without affecting performance. Every layer of a ResNet is composed of several blocks, and when ResNets go deeper, they normally do it by adding more blocks. The skip connection connects activations of a layer to further layers by skipping one or more layers in between. The architecture of ResNet50 has 4 stages, with the number of filters in each layer being the same depending on the size of the output feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a1208",
   "metadata": {},
   "source": [
    "7. What do Skip Connections entail ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90783277",
   "metadata": {},
   "source": [
    "Skip connections are part of the neural networks that skip some of the neural network layers and feed the output of one layer as the input to another layer further down in the network. The core idea is to backpropagate through the identity function, by just using a vector addition. Skip connections allow for increasing network depth without hurting performance, as first layers converge on low-level features while later layers learn high-level features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5c02f",
   "metadata": {},
   "source": [
    "8. What is the definition of a residual Block ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59594ef8",
   "metadata": {},
   "source": [
    "A residual block is a stack of layers in which the output of a layer is taken and added to another layer deeper in the block. The non-linearity is then applied after adding it together with the output of the corresponding layer in the main path. Residual blocks allow memory (or information) to be passed across layers without modification, which helps prevent vanishing gradients and allows for deeper networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e6c31",
   "metadata": {},
   "source": [
    "9. How can transfer learning help with problems ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af237e",
   "metadata": {},
   "source": [
    "Transfer learning can help with problems by allowing developers to take a blended approach from different models to fine-tune a solution to a specific problem. It is the idea of overcoming the isolated learning paradigm and utilizing knowledge acquired for one task to solve related ones. Instead of training a neural network from scratch, many pre-trained models can serve as the starting point for training. Transfer learning can save time, reduce computational resources, improve accuracy, and prevent overfitting by leveraging existing knowledge and experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d54f8",
   "metadata": {},
   "source": [
    "10. What is transfer learning, and how does it work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032332aa",
   "metadata": {},
   "source": [
    "Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. It involves taking the relevant parts of a pre-trained machine learning model and applying it to a new but similar problem. Transfer learning allows rapid progress when modeling the second task, saves time, reduces computational resources, improves accuracy, and prevents overfitting by leveraging existing knowledge and experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5301a2c",
   "metadata": {},
   "source": [
    "11.HOW DO NEURAL NETWORKS LEARN FEATURES? 11. HOW DO NEURAL NETWORKS LEARN FEATURES ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb1211",
   "metadata": {},
   "source": [
    "Neural networks learn features by initially processing several large sets of labeled or unlabeled data. They perform supervised learning tasks, building knowledge from data sets where the right answer is provided in advance. Neural networks process unknown inputs more accurately by using these examples. Convolutional neural networks learn abstract features and concepts from raw image pixels. They work by propagating forward inputs, weights, and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021083",
   "metadata": {},
   "source": [
    "12. WHY IS FINE-TUNING BETTER THAN START-UP TRAINING ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183b5dd",
   "metadata": {},
   "source": [
    "Fine-tuning is better than start-up training because it builds on knowledge (parameters) an existing model has learned from previous data. With fine-tuning, we first change the last layer to match the classes in our dataset and then unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers. This allows us to re-train (or \"fine-tune\") the weights of the top layers of the pre-trained model with a very low learning rate, which can increase its performance even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8debe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
