{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab57d8e",
   "metadata": {},
   "source": [
    "# CV_Assignment-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea0a97",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60abf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork(): \n",
    "    def __init__(self):  \n",
    "        np.random.seed(1)\n",
    "        self.weights = 2 * np.random.random((3, 1)) - 1. \n",
    "    def sigmoid(self, x): \n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb7ae49",
   "metadata": {},
   "source": [
    "2. Write the Python code to implement ReLU ?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d39122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d84c4",
   "metadata": {},
   "source": [
    "3. Write the Python code for a dense layer in terms of matrix multiplication ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53776d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d6589",
   "metadata": {},
   "source": [
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54bd0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = [[np.random.randn() for _ in range(output_size)] for _ in range(input_size)]\n",
    "        self.biases *=output_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = [sum([inputs[j] * self.weights[i][j] for j in range(len(inputs))]) + self.biases[i] for i in range(len(self.biases))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41721e8b",
   "metadata": {},
   "source": [
    "5. What is the “hidden size” of a layer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58d0c2",
   "metadata": {},
   "source": [
    "The \"hidden size\" of a layer is the number of learned parameters or the network's memory. It is usually defined by the user depending on the problem at hand, as using more units can make it more likely to overfit the training data. In RNNs, the hidden size is the number of features of the hidden state. The input embedding size and hidden size do not have to be the same. In PyTorch, defining a simple 1-hidden-layer neural network for classification on MNIST requires specifying a parameter that determines the hidden size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca947e0",
   "metadata": {},
   "source": [
    "6. What does the t method do in PyTorch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c46b7",
   "metadata": {},
   "source": [
    "The t method in PyTorch transposes dimensions 0 and 1 of a tensor. It expects the input to be a 2-D tensor or less, and returns 0-D and 1-D tensors as is. The log_prob method returns the log of the probability density/mass function evaluated at the given sample value. The training_step() method is required to activate the training loop in PyTorch Lightning. PyTorch implements gradient-based optimization methods in torch.optim, including Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557b672",
   "metadata": {},
   "source": [
    "7. Why is matrix multiplication written in plain Python very slow ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7532195",
   "metadata": {},
   "source": [
    "Matrix multiplication written in plain Python is very slow because it involves nested loops that iterate over each element of the matrices. This results in a time complexity of O(n^3), which can be very slow for large matrices. In contrast, optimized libraries like NumPy use highly optimized C code and BLAS libraries to perform matrix multiplication much faster than plain Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a31c596",
   "metadata": {},
   "source": [
    "8. In matmul, why is ac==br ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee318e69",
   "metadata": {},
   "source": [
    "In matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. Therefore, if a is an a x b matrix and b is a c x d matrix, then the resulting product will be an a x d matrix. In other words, for two matrices to be multiplied together using matmul, their inner dimensions must match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91c9e4",
   "metadata": {},
   "source": [
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa902f",
   "metadata": {},
   "source": [
    "In Jupyter Notebook, you can use the magic commands %timeit and %%timeit to measure the execution time of your code. %timeit measures the execution time of a single statement multiple times and returns the average execution time. %%timeit measures the execution time of an entire cell multiple times and returns the average execution time. The Execute Time extension displays when the last execution of a code cell occurred, and how long it took"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083542b",
   "metadata": {},
   "source": [
    "\n",
    "10. What is elementwise arithmetic ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca9eaf",
   "metadata": {},
   "source": [
    "Elementwise arithmetic is an operation between two tensors that operates on corresponding elements within the respective tensors. It is obtained by operating on one element of a matrix at a time. The Hadamard product, also known as the element-wise product, entrywise product or Schur product, is a binary operation that takes two matrices and produces another matrix, where each element is the product of the corresponding elements in the original matrices. Other functions like max, min, median, mean, std, sum and prod take a vector and return its maximum, minimum, median, arithmetic mean, standard deviation and element sum respectively. Element-wise operations include addition, subtraction, multiplication, division and exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b3a24",
   "metadata": {},
   "source": [
    "11.Write the PyTorch code to test whether every element of a is greater than the corresponding element of b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2457b",
   "metadata": {},
   "source": [
    "import torch            \n",
    "\n",
    "a = torch.tensor([1, 2, 3])            \n",
    "b = torch.tensor([0, 2, 4])              \n",
    "\n",
    "greater_than_b = a > b                 \n",
    "print(torch.all(greater_than_b))                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65d4e1",
   "metadata": {},
   "source": [
    "12.What is a rank-0 tensor? How do you convert it to a plain Python data type ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38016b79",
   "metadata": {},
   "source": [
    "A rank-0 tensor is a scalar or a tensor that contains a single value and no axes. To convert a rank-0 tensor to a plain Python data type, you can use the numpy() method in TensorFlow 2.0+ or later. Alternatively, most object classes like NumPy's ndarray, TensorShape, Python lists and tf.Variable will all convert automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55095462",
   "metadata": {},
   "source": [
    "13.How does elementwise arithmetic help us speed up matmul ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed353d1b",
   "metadata": {},
   "source": [
    "Elementwise arithmetic can help speed up matmul by eliminating loops and replacing them with PyTorch functionalities. This gives C speed performance without the need for explicit loops. Elementwise multiplication is faster than matrix multiplication because it involves fewer operations and does not require the same level of memory access. However, extremely complex element-wise operations may have negligible performance impact when compared to a slow matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3483df",
   "metadata": {},
   "source": [
    "14.What are the broadcasting rules ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3083738",
   "metadata": {},
   "source": [
    "The broadcasting rules in PyTorch are based on NumPy broadcasting semantics. Two tensors are compatible for broadcasting only if, when starting from the trailing dimensions of the tensors, their dimensions are equal or one of them is 1. If the dimensions of two tensors do not match, then the smaller tensor will be broadcasted to match the shape of the larger tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29327f0c",
   "metadata": {},
   "source": [
    "15.What is expand_as? Show an example of how it can be used to match the results of broadcasting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b6430",
   "metadata": {},
   "source": [
    "import torch                           \n",
    "\n",
    "a = torch.tensor([1, 2, 3])               \n",
    "b = torch.tensor([,[2], [4]])               \n",
    "\n",
    "expanded_a = a.expand_as(b)               \n",
    "print(expanded_a)              \n",
    "\n",
    "broadcasted_a = a.unsqueeze(1).expand_as(b)               \n",
    "print(broadcasted_a)                \n",
    "\n",
    "print(torch.all(expanded_a == broadcasted_a))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08b655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
