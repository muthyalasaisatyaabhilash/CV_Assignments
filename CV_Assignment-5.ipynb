{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d70681",
   "metadata": {},
   "source": [
    "# CV_Assignment-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9523c",
   "metadata": {},
   "source": [
    "1. How can each of these parameters be fine-tuned? • Number of hidden layers\n",
    "• Network architecture (network depth)\n",
    "\n",
    "• Each layer&#39;s number of neurons (layer width)\n",
    "\n",
    "• Form of activation\n",
    "\n",
    "• Optimization and learning\n",
    "\n",
    "• Learning rate and decay schedule\n",
    "\n",
    "• Mini batch size\n",
    "\n",
    "• Algorithms for optimization\n",
    "\n",
    "• The number of epochs (and early stopping criteria)\n",
    "\n",
    "• Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "• L2 normalization\n",
    "\n",
    "• Drop out layers\n",
    "• Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2755ce5",
   "metadata": {},
   "source": [
    "The number of hidden layers, network architecture (network depth), each layer's number of neurons (layer width), form of activation, and optimization can be fine-tuned. Fine-tuning parameters use a smaller learning rate, while training the output layer from scratch can use a larger learning rate. The process involves using an already trained network and re-training part of it with new data sets. Techniques learned here can be used to finetune Alexnet, Inception, Resnet or any other networks      \n",
    "The learning rate and decay schedule, mini-batch size, algorithms for optimization, the number of epochs (and early stopping criteria) can be configured. Perhaps the simplest learning rate schedule is to decrease the learning rate linearly from a large initial value. A rather simple fix for this dilemma is to use a warmup period during which the learning rate increases to its initial maximum and to cool down the rate until. AutoLRS is an algorithm that tunes learning rate schedules based on Bayesian Optimization with Gaussian processes and loss curve forecasting. It is common practice to decay the learning rate. However, one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size\n",
    "\n",
    "\n",
    "Overfitting can be avoided by using regularization techniques such as L2 normalization, drop out layers, and data augmentation. Hyperparameters related to network structure, such as the number of hidden units and how the network is trained, can be tuned. Tuning the regularization parameter of a neural net (L2 regularization) can be done using a grid search with values such as 0.0005, 0.005, 0.05, 0.5, and 5. A regularization hyperparameter controls the capacity of the model and how many degrees of freedom it has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b8c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
