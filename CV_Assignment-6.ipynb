{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159eb2ee",
   "metadata": {},
   "source": [
    "# CV_Assignment-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daf0e7",
   "metadata": {},
   "source": [
    "1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136f2df",
   "metadata": {},
   "source": [
    "Trainable parameters are those whose values are optimized during training as per their gradient Non-trainable parameters are those whose values are not updated during training with backpropagation, such as hyperparameters like the weights used in pooling layers. The cosine similarity between document embeddings can tell how much features a model has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576b631",
   "metadata": {},
   "source": [
    "2. In the CNN architecture, where does the DROPOUT LAYER go ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11f462",
   "metadata": {},
   "source": [
    "A dropout layer can be applied to the input vector or a hidden layer. Dropout layers are usually placed after the activation function of each convolutional layer. The term “dropout” refers to dropping out the nodes (input and hidden layer) in a neural network Dropout is a technique where randomly selected neurons are ignored during training, meaning that their contribution to the subsequent layer is eliminated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad28c6e0",
   "metadata": {},
   "source": [
    "3. What is the optimal number of hidden layers to stack ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e302220",
   "metadata": {},
   "source": [
    "The optimal number of hidden layers to stack depends on the complexity and dimensions of the data. If data is less complex and has fewer dimensions or features, then neural networks with 1 to 2 hidden layers would work. If data has large dimensions or features, then 3 to 5 hidden layers can be used. The number of hidden neurons should be between the size of the input layer and the size of the output layer, with a minimum number of layers being 0 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b9291",
   "metadata": {},
   "source": [
    "4. In each layer, how many secret units or filters should there be ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec8d87",
   "metadata": {},
   "source": [
    "The number of secret units or filters in each layer depends on the network's depth and complexity. Usually, we start with a small number of filters at the initial layers and progressively increase the count as we go deeper into the network. The number of filters is equal to the number of neurons, since each neuron performs a different convolution on the input to the layer. The network structures like the number of hidden units are managed by CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c405b",
   "metadata": {},
   "source": [
    "5. What should your initial learning rate be ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dda318",
   "metadata": {},
   "source": [
    "The initial learning rate should be a small positive value, often in the range between 0.1 and 0.0001. The default learning rate is 0.01 and no momentum is used by default. The optimal learning rate can be found using various methods such as grid search, random search, or cyclical learning rates. The learning rate is explained as the magnitude of change/update to model weights during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80e1d0",
   "metadata": {},
   "source": [
    "6. What do you do with the activation function ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc8c31",
   "metadata": {},
   "source": [
    "The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. It is used to get the output of a node in a neural network. The activation function of a node defines the output of that node given an input or set of inputs. An activation function in a neural network transforms the weighted sum of the input into an output from a node or nodes in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65977b",
   "metadata": {},
   "source": [
    "7. What is NORMALIZATION OF DATA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf27375",
   "metadata": {},
   "source": [
    "Data normalization is the process of organizing data to appear similar across all records and fields. It increases the cohesion of entry types. Normalization can have a range of meanings in statistics and applications of statistics, such as adjusting ratings or transforming data into a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff50516",
   "metadata": {},
   "source": [
    "8. What is IMAGE AUGMENTATION and how does it work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970caf84",
   "metadata": {},
   "source": [
    "Image augmentation is a technique that can increase the size of the training set without acquiring new images by duplicating images with some kind of variation so the model can learn from more example. Image augmentation algorithms include geometric transformations, color space augmentations, kernel filters, mixing, and more. It is used to artificially expand the size of a training dataset by creating modified versions of images. Deep networks need large amounts of training data to achieve good performance. To build a powerful image classifier using very little training data, image augmentation can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc819f36",
   "metadata": {},
   "source": [
    "9. What is DECLINE IN LEARNING RATE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f836d",
   "metadata": {},
   "source": [
    "Decline in learning rate refers to the process of reducing the learning rate during training by adjusting it according to a pre-defined schedul. The most popular form of learning rate annealing is a step decay where the learning rate is reduced by some percentage after a set number of epoch. If you pick a learning rate that is too small, learning will take too long. There's a Goldilocks learning rate for every regression problem. In setting a learning rate, there is a trade-off between the rate of convergence and overshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc0ad1",
   "metadata": {},
   "source": [
    "10. What does EARLY STOPPING CRITERIA mean ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972ed19",
   "metadata": {},
   "source": [
    "Early stopping is an optimization technique used to reduce overfitting without compromising on model accuracy. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Some important parameters of the early stopping callback include monitor, which is the quantity to be monitored, and min_delta, which is the minimum change in the monitored quantity that qualifies as an improvement. Regularization by early stopping can be done either by dividing the dataset into training and test sets and then using cross-validation on them or by monitoring a validation set while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9454192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
