{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfdc3aa",
   "metadata": {},
   "source": [
    "# CV_Assignment-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee9498",
   "metadata": {},
   "source": [
    "1. Describe the Quick R-CNN architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c6d8a",
   "metadata": {},
   "source": [
    "Quick R-CNN is an object detection model that uses a convolutional neural network (CNN) to extract features from an image and then applies a region of interest (ROI) pooling layer to extract a fixed-length feature vector from each region proposal. However, the question asks for the Quick R-CNN architecture, which is not provided in the search results. Instead, the search results provide information on Faster R-CNN and Fast R-CNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0337e04",
   "metadata": {},
   "source": [
    "2. Describe two Fast R-CNN loss functions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43320f17",
   "metadata": {},
   "source": [
    "The Fast R-CNN loss function consists of two parts: a classification loss and a regression loss. The classification loss is the predicted probability for the proposed region belonging to class c, which is the ground truth class. The regression loss measures the difference between the predicted bounding box and the ground truth bounding box. Therefore, two possible Fast R-CNN loss functions are classification and regression losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b9708",
   "metadata": {},
   "source": [
    "3. Describe the DISABILITIES OF FAST R-CNN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de0e84",
   "metadata": {},
   "source": [
    "The search results do not provide information on the disabilities of Fast R-CNN. Instead, they provide information on Faster R-CNN and how it performs object detection using region-proposal networks. Therefore, it is impossible to describe the disabilities of Fast R-CNN based on the given search results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4de31b",
   "metadata": {},
   "source": [
    "4. Describe how the area proposal network works ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0ec5",
   "metadata": {},
   "source": [
    "The Area Proposal Network (RPN) is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. It is trained end-to-end to generate high-quality region proposals, which are then used for object detection in the RCNN family. The RPN uses anchor boxes with various scales and aspect ratios to generate proposals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b9f50",
   "metadata": {},
   "source": [
    "5. Describe how the RoI pooling layer works ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f04ebd",
   "metadata": {},
   "source": [
    "The ROI pooling layer produces fixed-size feature maps from non-uniform inputs by doing max-pooling on the inputs. The number of output channels is equal to the number of object classes plus one for background. The ROI pooling layer is used because the fully-connected layer always expects the same input size, but input regions to the FC layer may have different sizes. Therefore, it is necessary to use a pooling operation that can produce a fixed-size output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a3ea1",
   "metadata": {},
   "source": [
    "6. What are fully convolutional networks and how do they work? (FCNs) ?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900be86",
   "metadata": {},
   "source": [
    "Fully Convolutional Networks (FCNs) are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as convolution, pooling, and upsampling. FCNs do not contain any \"Dense\" layers as in traditional CNNs. Instead, they contain 1x1 convolutions that perform the same function as dense layers. The fully convolutional network first uses a CNN to extract image features and then transforms the number of channels into the number of classes via a 1 Ã— 1 convolution layer. Essentially, convolution is the first layer that extracts features from an input image by performing matrix multiplication of the image matrix and a learnable kernel matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ba775",
   "metadata": {},
   "source": [
    "7. What are anchor boxes and how do you use them ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1199a8",
   "metadata": {},
   "source": [
    "Anchor boxes are a set of predefined bounding boxes of a certain height and width. They are used to capture the scale and aspect ratio of specific object classes that you want to detect, and are typically chosen based on object sizes in your training datasets. In object detection models, anchor boxes are used to make bounding box predictions. The labels for locations of ground-truth bounding boxes and classes of their surrounded objects can be used to generate multiple anchor boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2bf47",
   "metadata": {},
   "source": [
    "8. Describe the Single-shot Detector's architecture (SSD) ?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbddb9",
   "metadata": {},
   "source": [
    "The Single-Shot Detector (SSD) is an object detection model that has two components: a backbone model and SSD head. The backbone model is usually a pre-trained image classification network, such as VGG or ResNet. SSD predicts the boundary boxes and the classes directly from feature maps in one single shot. It discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345b782",
   "metadata": {},
   "source": [
    "9. HOW DOES THE SSD NETWORK PREDICT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514dd64a",
   "metadata": {},
   "source": [
    "The SSD network predicts the boundary boxes and the classes directly from feature maps in one single shot. It uses a matching phase while training to match the appropriate anchor box with the bounding boxes of each ground truth object within an image. The anchor box with the highest degree of overlap with an object is responsible for predicting that object's class and its location. The SSD model adds several feature layers to the end of a base network, which predicts the offsets to default boxes of different scales and aspect ratios. Therefore, it is possible to predict multiple objects at different scales using multiscale feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756234ab",
   "metadata": {},
   "source": [
    "10. Explain Multi Scale Detections ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3961ac",
   "metadata": {},
   "source": [
    "Multi-scale detections refer to the use of multiple layers or feature maps to detect objects independently. The SSD model captures multi-scale features using a pyramidal feature hierarchy. It aggregates context information from multiple scales to improve the accuracy of single-shot detection. Therefore, it is possible to detect objects at different scales and aspect ratios using multiscale feature maps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a260e2",
   "metadata": {},
   "source": [
    "11. What are dilated (or atrous) convolutions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255338bd",
   "metadata": {},
   "source": [
    "Dilated (or atrous) convolutions are a type of convolution that \"inflate\" the kernel by inserting holes between its consecutive elements. This technique expands the kernel and skips some of the points, allowing for larger receptive fields without increasing the number of parameters. The dilation factor controls the spacing between the elements in the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5335c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
