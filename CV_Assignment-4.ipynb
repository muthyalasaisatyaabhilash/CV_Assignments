{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb55fc4",
   "metadata": {},
   "source": [
    "# CV_Assignment-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9672f7",
   "metadata": {},
   "source": [
    "1. What is the concept of cyclical momentum ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ed401",
   "metadata": {},
   "source": [
    "Cyclic momentum is a learning rate scheduling technique for faster training of a network. Momentum and learning rate are closely related, and the weight update equation for SGD shows that momentum has a similar impact as learning rate. The cycle of momentum refers to the process of activating the public and changing the political weather by using popular movements. A cyclical process is one in which a series of events happens again and again in the same order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c44d21",
   "metadata": {},
   "source": [
    "2. What callback keeps track of hyperparameter values (along with other data) during training ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffb8ca",
   "metadata": {},
   "source": [
    "The CSVLogger callback function helps to keep track of metric values for each epoch during training. It is useful for managing hyperparameters and passing them to ML scripts. The EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset. To continually monitor and keep track of results while tuning hyper-parameters, three approaches can be used: logging, checkpointing, and early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401339b6",
   "metadata": {},
   "source": [
    "3. In the color dim plot, what does one column of pixels represent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a401a4b",
   "metadata": {},
   "source": [
    "In the color dim plot, one column of pixels represents a vertical line of pixels in an image. Each pixel is represented by a number or set of numbers, and the range of these numbers is called the color depth or bit depth. The output of the np.histogram function is a one-dimensional NumPy array with 256 rows and one column representing the number of pixels with the corresponding intensity value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e040b9d",
   "metadata": {},
   "source": [
    "4. In color dim, what does \"poor teaching\" look like? What is the reason for this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca739a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a42f1a4a",
   "metadata": {},
   "source": [
    "5. Does a batch normalization layer have any trainable parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0730d",
   "metadata": {},
   "source": [
    "Yes, batch normalization adds two trainable parameters to each layer. In addition to the weights and bias of any network layer, a Batch Norm layer also has parameters of its own. The purpose of introducing learnable/trainable parameters in batch normalization is to normalize inputs to the next layers. Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6826e",
   "metadata": {},
   "source": [
    "6. In batch normalization during preparation, what statistics are used to normalize? What about during the validation process ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436aab0",
   "metadata": {},
   "source": [
    "Batch normalization relies on batch first and second statistical moments (mean and variance) to normalize hidden layers activations during preparation. The output values are then scaled and shifted using learned parameters. During the validation process, the statistics used to normalize are not computed from the current batch but rather from a moving average of past batches. Scaling data into [0, 1] will result in slow learning, so standardizing inputs to a network is still necessary even when using batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf003dcc",
   "metadata": {},
   "source": [
    "7. Why do batch normalization layers help models generalize better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1b32c",
   "metadata": {},
   "source": [
    "Batch normalization solves a major problem called internal covariate shift, which helps by making the data flowing between intermediate layers of the neural network look consistent. This means that a higher learning rate can be used, and it has a regularizing effect which means dropout can often be removed. Batch normalization makes training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling them. Overall, batch normalized models achieve higher validation and test accuracies on all datasets, which is attributed to its regularizing effect and more stable optimization process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0886c",
   "metadata": {},
   "source": [
    "8.Explain between MAX POOLING and AVERAGE POOLING is number eight ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc1695",
   "metadata": {},
   "source": [
    "Max pooling selects the brighter pixels from the image, which is useful when the background of the image is dark and we are interested in only the lighter pixels of the image. Average pooling method smooths out the image and hence sharp features may not be identified when this pooling method is used. Average pooling encourages the network to identify the complete extent of an object, whereas max pooling restricts that to only very important features. Max-pooling computes by taking the maximum activation in a block, while average pooling computes by taking the average of elements present in a region of feature map covered by a filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20005f",
   "metadata": {},
   "source": [
    "9. What is the purpose of the POOLING LAYER ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6bffa",
   "metadata": {},
   "source": [
    "The pooling layer summarizes the features present in a region of the feature map generated by a convolution layer. It reduces the dimensions of the feature maps, which reduces the number of parameters to learn and the amount of computation performed in the network. The purpose of pooling layers is to reduce the size of feature maps and make computations more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309fe65",
   "metadata": {},
   "source": [
    "10. Why do we end up with Completely CONNECTED LAYERS ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85499cab",
   "metadata": {},
   "source": [
    "Fully connected networks are the workhorses of deep learning, used for thousands of applications. The output from the convolutional layers represents high-level features in the data. While that output could be flattened and connected to a classification output layer, adding one or more fully connected layers is a popular approach. Fully connected layers are global and can introduce any kind of dependence, which is why they work so well in domains like image recognition. A fully connected layer refers to a neural network in which each neuron applies a linear transformation to the input vector through weights. Fully connected layers are an essential component of Convolutional Neural Networks (CNNs), which have been proven very successful in recognizing and classifying images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8336cc",
   "metadata": {},
   "source": [
    "11. What do you mean by PARAMETERS ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfecf7",
   "metadata": {},
   "source": [
    "Parameters in machine learning and deep learning are the values your learning algorithm can change independently as it learns, and these values are affected by the choice of hyperparameters you provide. Model parameters are something that a model learns on its own, such as weights or coefficients of independent variables in linear regression. A model parameter is a configuration variable that is internal to the model and whose value can be estimated from the given data. The machine learning model parameters determine how input data is transformed into the desired output, whereas hyperparameters control the model's shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf83f76",
   "metadata": {},
   "source": [
    "12. What formulas are used to measure these PARAMETERS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b08d0",
   "metadata": {},
   "source": [
    "There are many metrics to determine the performance of machine learning models, such as accuracy, precision, recall, F1 score, AUC-ROC. The formulas used to measure these parameters depend on the metric being used. For example, to calculate the Total Sum of Squares (TSS), which is proportional to the variance of the test set target values, you can use this formula: TSS = sum((y_test - y_test.mean())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4dd84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
