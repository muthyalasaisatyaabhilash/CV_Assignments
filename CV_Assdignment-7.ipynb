{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8bd863",
   "metadata": {},
   "source": [
    "# CV_Assdignment-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203df932",
   "metadata": {},
   "source": [
    "1. What is the COVARIATE SHIFT Issue, and how does it affect you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fdede",
   "metadata": {},
   "source": [
    "Covariate shift is a problem that occurs when the distribution of variables in the training data is different from that of real-world or testing data. This can cause models to make incorrect predictions and lower their accuracy. The covariate-shift method attempts to adjust the domain of applicability so that it is more aligned with the prediction set. During gradient descent training, the layer's weights and obtained data distributions can vary, making learning difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182823a",
   "metadata": {},
   "source": [
    "2. What is the process of BATCH NORMALIZATION?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb4c43",
   "metadata": {},
   "source": [
    "Batch normalization is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015. The technique standardizes the inputs to a network, applied to either the activations of a prior layer or inputs directly. Batch-Normalization (BN) is an algorithmic method that makes the training of Deep Neural Networks (DNN) faster and more stable. It consists of normalizing interlayer outputs into a standard format called normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0235cc",
   "metadata": {},
   "source": [
    "3. Using our own terms and diagrams, explain LENET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c4c00",
   "metadata": {},
   "source": [
    "The LeNet-5 architecture is a classic convolutional neural network (CNN) that consists of seven layers. The first two sets of layers are convolutional and average pooling layers, followed by a flattening convolutional layer. Then there are two fully-connected layers and finally a softmax classifier. The architecture contains the basic modules of deep learning: convolutional layer, pooling layer, and full link layer. LeNet-5 is made up of three convolutional layers, two subsampling layers, and two fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b022e35",
   "metadata": {},
   "source": [
    "4. Using our own terms and diagrams, explain ALEXNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2f458",
   "metadata": {},
   "source": [
    "PERPLEXITY\n",
    "AlexNet is a convolutional neural network (CNN) that was designed by Hinton and his student Alex Krizhevsky. It was the first CNN to win Image Net, and it used GPU to boost performance. The architecture consists of eight layers, with the first five being convolutional and the last three being fully connected layers. In between, there are some 'layers' called pooling and normalization layers. The architecture contains 60 million parameters and 650,000 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864b74a",
   "metadata": {},
   "source": [
    "5. Describe the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71e679",
   "metadata": {},
   "source": [
    "The vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods. When there are more layers in the network, the value of the product of derivative decreases until at some point the partial derivative of the loss function approaches a value close to zero, and the partial derivative vanishes. A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. This can cause problems with recurrent neural networks as well since their update involves unrolling the network for each. The problem is encountered while training Neural Networks with gradient-based methods such as Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f91568",
   "metadata": {},
   "source": [
    "6. What is NORMALIZATION OF LOCAL RESPONSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24f0cd",
   "metadata": {},
   "source": [
    "Local Response Normalization (LRN) is a normalization layer that implements the idea of lateral inhibition. Lateral inhibition is a concept in neurobiology that refers to the capacity of an excited neuron to reduce the activity of its neighbors. LRN was first introduced in AlexNet architecture where the activation function used was ReLU as opposed to tanh and sigmoid at that time. The reason for using LRN was to encourage lateral inhibition. The local response normalization layer performs a kind of “lateral inhibition” by normalizing over local input regions. It is a normalization layer that makes use of lateral inhibition to enforce the notion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9b3a2",
   "metadata": {},
   "source": [
    "7. In AlexNet, what WEIGHT REGULARIZATION was used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95b909",
   "metadata": {},
   "source": [
    "In AlexNet, weight decay was used with a value of 0.0005. Weight regularization provides an approach to reduce the overfitting of a deep learning neural network model on the training data and improve generalization. It is achieved by adding a regularization term to the loss function. Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weights is simpler and has less chance of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317a8ba",
   "metadata": {},
   "source": [
    "8. Using our own terms and diagrams, explain VGGNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db9c43",
   "metadata": {},
   "source": [
    "VGGNet is a standard deep Convolutional Neural Network (CNN) architecture with multiple layers. The “deep” refers to the number of layers, with VGG-16 or VGG-19 consisting of 16 and 19 convolutional layers. The overall structure includes five sets of convolutional layers, followed by a few fully connected layers. It was based on an analysis of how to increase the depth of such networks. The VGGNet architecture incorporates the most important convolution neural network features and has been around for a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397e59f",
   "metadata": {},
   "source": [
    "9. Describe VGGNET CONFIGURATIONS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883b449",
   "metadata": {},
   "source": [
    "VGGNet has five configurations named A to E. The depth of the configuration increases from left (A) to right (E). Each configuration consists of convolutional layers, followed by a few fully connected layers. The convolutional layer parameters are denoted as C-N-F, where N is the number of filters and F is the filter size. VGG16 refers to the VGG model, also called VGGNet. It is a convolution neural network (CNN) model with 16 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db02d03",
   "metadata": {},
   "source": [
    "10. What regularization methods are used in VGGNET to prevent overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f9f09",
   "metadata": {},
   "source": [
    "Regularization methods such as dropout, L1 regularization, and L2 regularization are used in VGGNet to prevent overfitting. One key aspect of the regularization methods is to prevent the model from overfitting the training data. Other techniques to prevent overfitting include simplifying the model, early stopping, data augmentation, and dropout. Large and Deep Convolutional Neural Networks achieve good results in image classification tasks but need methods to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f567c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
